
\chapter{Architecture}

\section{Tokenization}

\subsection{Logic}

\paragraph{UTF-8} 
The MSMARCO corpus contains several wrongly encoded documents, it seems their \textit{spider} wrongly interpreted certain documents as encoded in \texttt{latin1} --- a eight bit ASCII extension used by old Microsoft's software --- instead of \texttt{UTF-8} and then converted them in \texttt{UTF-8} interpreting their bytes as \texttt{UTF-8} data. To solve this issue we wrote a simple heuristic that tries to detect strange Unicode's entry-points that are likely the result of the processes; and a reverse encoding procedure that converts them back in their original \texttt{UTF-8} form. We didn't perform experiments so we cannot tell if this actually improves accuracy on the testing corpus.

\paragraph{Punctuation and splitting}
Our implementation takes a very simplistic approach to punctuation handling, that is all known western punctuation is removed and replaced with spaces, you can see the punctuation handled by the program in the source file \texttt{normalizer/PunctuationRemover.cpp}.

Words, that are tokens, are generated by splitting the resulting text by spaces.

\paragraph{Lower case}
Changing case of a Unicode word is a non trivial task and there are no fast ready-to-use libraries that directly operate on \texttt{UTF-8} --- instead they require a conversion to \texttt{UTF-16} or \texttt{UTF-32}, that we'd rather avoid, --- nevertheless we decided not to limit ourselves to only handle the case of ASCII's latin letters but we convert the case of \textit{latin1}'s latin letters --- that is a larger subset of Unicode latin letters. In conclusion all words written in latin characters are converted in lower-case.

\paragraph{Stemming}
Stemming is done using the Porter algorithm.

\subsection{Libraries}

\paragraph{Punctuation}
To replace punctuation with spaces we make use of regex expressions and run them on Hyperscan (or Vectorscan on non amd64 platforms), that is a fast regex engine that make use of SIMD instructions --- such as AVX-512 --- to speed up their solution.

\paragraph{Stemming}
To stem words, we make of the famous Snowball's stemmer implementation.

\section{Indices' organization}

\paragraph{}
We've designed the system to partition by document TODO TODO

\section{Indices}

bruh

\section{Lexica}

Since we're partitioning the dataset by document, to improve its parallelizability we make use of a local lexicon for each partition as well as a global lexicon used to store shared information.

\subsection{Disk Map}

To store the lexica it is necessary to use a disk-based data structure that
allows the program to quickly and efficiently seek the datum associated to a
particular key, that is an index term. To achieve this we make use of following:

\begin{itemize}
	\item to reduce disk seek time, the terms' data are compressed with
	\textit{integer compression} and the terms themselves with \textit{prefix compression}
	
	\item to minimize disk accesses, the terms are ordered lexicographically
	and partitioned in pages of the same size of the disk's page
	
	\item terms at the beginning of each page are stored in a separated array that fits in system memory, to quickly locate --- through \textit{binary search} or a \textit{trie visit} --- in which page a given term is located
\end{itemize}

\paragraph{Structure}
Let us call $|B|$ the size of a disk page, $|T|$ the maximum length off all terms stored, $m$ the maximum number of key-value pairs stored in a page, $k$ the number of pages used to store them all, $s_1 > s_2 > s_3 > \dots$ the terms to store and $d_1, d_2, d_3, \dots$ their data; and $s_{b_1} > s_{b_2} > \dots > s_{b_k}$ the terms that appear at the start of each page.

The following picture shows how our data are organized at high level

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{assets/disk_map_base}
	\caption[]{Data organization}
	\label{fig:diskmapbase}
\end{figure}

In the real implementation, the pages' term heads $s_{b_i}$ are only stored in the look-up table, the data $d_i$ are compressed using \textit{variable bytes} and the other strings $s_j$ are compressed by cutting the common prefix they've with the page's head, so a generic key-value pair is more like:
$\left<	\left< p(s_{b_i}, s_j), {s'}_j \right>, c(d_j)	\right>$; where $p()$ is the length of the common prefix, $ {s'}_j$ is the string without the common prefix, and $c(d_j)$ is the compressed datum. The prefix's length is stored on a eight bit unsigned number, thus allowing for a maximum term's length of 255 bytes.

\paragraph{Complexity}
For simplicity's sake our implementation makes use of binary search to find in which page a term is stored and a linear scan to find it in the page, giving us a time complexity of 
$O\left(	|T|\cdot\log(k) + m	\right)$,
a space complexity, with regard to system memory, of
$O\left(	|T|\cdot k	\right)$
--- since we only have to keep the heads in memory --- and an I/O complexity of
$O\left(	1	\right)$, assuming all pages' heads fit in memory.

\subsection{Data}

\paragraph{Local lexica}
Each local lexicon stores the terms' information as \texttt{LexiconValue} during the first pass and then as \texttt{SigmaLexiconValue} in the second pass, when the upper bounds are computed and the skipping lists built.

Both structures store the number of docs in which such term appears and pointers to the compressed term frequencies and documents' ids. The latter structure also contains the skipping list and the upper bounds for \textit{TFIDF} and \textit{BM25}.

\paragraph{Global lexicon}
The only global lexicon just stores the total number of documents each term appear in, this is necessary to correctly compute the \textit{IDF} term.

